---
title: 第八讲——并发编程之线程二
id: 8
date: 2019-8-23 20:00:00
tags: socket和并发编程
comment: true
---

### 学习大纲

- 死锁现象与递归锁
- 信号量
- GIL全局锁解释器
- GIL与lock的区别
- 验证计算密集型和IO密集型的效率
- 多线程实现socket通信
- 进程池、线程池

<!----more---->

### 死锁现象与递归锁

**什么是死锁**

- 死锁就是指两个或者两个以上的进程或者线程在执行的过程中，因抢夺资源而造成的一种互相等待的现象，若无外力作用，他们都将无法推进下去。此时称系统处于死锁状态或系统产生了死锁，这些永远在相互等待的进程称之为死锁进程。

**死锁现象**

实例代码

```python
#死锁现象
from threading import RLock,Lock
import time

lock_A = Lock()
lock_B = Lock()

class MyThread(Thread):
	def run(self):
		self.t1()
		self.t2()

	def t1(self):
		lock_A.acquire()
		print (f"{self.name}拿到了A")
		lock_B.acquire()
		print(f"{self.name}拿到了B")
		lock_B.release()
		lock_A.release()
	def t2(self):
		lock_B.acquire()
		print(f"{self.name}拿到了B")
		time.sleep(0.1)  #加了一个很短的阻塞就产生了死锁
		lock_A.acquire()
		print(f"{self.name}拿到了A")
		lock_A.release()
		lock_B.release()


if __name__ == '__main__':
	for i in range(3):
		t = MyThread()
		t.start()
结果：
Thread-1拿到了A
Thread-1拿到了B
Thread-1拿到了B
Thread-2拿到了A
'''
代码分析：（前提：2把锁，3个人抢）
T1先抢到了A锁，此时T2和T3也想抢A锁，但是只能等待T1释放A锁，
T1又抢到了B锁，此时T1就有A，B两把锁，并且没有释放，T2和T3就
继续等待，当T1依次释放B、A锁时，线程T2和T3就争抢A锁，T1争抢B锁；按照结果分析，T2抢到了A锁，T1抢到了B锁，但是接下来，T1睡了0.1秒，T1的拥有B锁，想要A锁，T2拥有A锁，想要B锁，但是他们谁都得不到对方的锁，所以他们都等待对方的结束，自然程序就会在这里阻塞住。
'''
```

**递归锁（解决死锁情况）**

- 介绍：递归锁可以解决死锁现象，业务需要多个锁时，先要考虑递归锁。

- 递归锁的原理：递归锁有一个计数的功能，原数字为0，上一次锁，计数+1，释放一锁技术-1。只要递归锁上面的数字不为0，其他线程就不能 抢锁。

- 官方原理：这个RLock内部维护着一个Lock和一个counter变量，counter记录了acquire的次数，从而使得资源可以被多次require。直到一个线程所有的acquire都被release，其他的线程才能获得资源。

```python
#重构上面的代码
from threading import RLock,Lock
import time

lock_A = RLock()
lock_B = lock_A  #固定的写法

class MyThread(Thread):
	def run(self):
		self.t1()
		self.t2()

	def t1(self):
		lock_A.acquire()
		print(f"{self.name}拿到了A")
		lock_A.acquire()
		print(f"{self.name}拿到了B")
		lock_B.release()
		lock_A.release()

	def t2(self):
		lock_B.acquire()
		print(f"{self.name}拿到了B")
		time.sleep(2)
		lock_A.acquire()
		print(f"{self.name}拿到了A")
		lock_A.release()
		lock_B.release()


if __name__ == '__main__':
	for i in range(3):
		t = MyThread()
		t.start()
```

注意：进程也有死锁和递归锁，进程的死锁和递归锁与线程的死锁和递归锁同理。

### 信号量（Semaphore）

功能：同一时刻可以设置抢锁的线程或者进程的数量。（就是控制并发的数量）

原理：Semaphore管理一个内置的计数器，每当调用acquire()时内置计数器-1；调用release()时内置计数器+1；计数器不能小于0；当计数器为0时，acquire()将阻塞线程直到其他线程调用release()。

实例代码

```python
#同时只有5个线程可以获得semaphore，即可以限制最大连接数为5
from threading import Thread
from threading import Semaphore
from threading import current_thread
import time
import random

sem = Semaphore(5)

def go_public_wc():
    sem.acquire()
    print(f'{current_thread().getName()} 上厕所ing')
    time.sleep(random.randint(1,3))
    sem.release()


if __name__ == '__main__':
    for i in range(20):
        t = Thread(target=go_public_wc)
        t.start()
分析：
#尽管我们在进程里面开了20个线程，但是我们设置了信号量为5，所以我么每次并发的最大数量最多不能超过5个。
```

### GIL全局锁解释器

理论上说：单个线程的多线程是可以利用多核的。

![](http://9017499461.linshutu.top/%E7%A8%8B%E5%BA%8F%E7%9A%84%E6%89%A7%E8%A1%8C%E6%B5%812.jpg)

实际上：开发Cpython解释器的程序员，给进入解释器的线程加了锁，同一时刻只能有一个进程进入解释器，Cpython独有的。

**为什么加锁？**

- 开发解释器的时候，当时都是单核时代，而且cpu的价格非常贵。
- 如果不加全局解释器锁，开发Cpython解释器的程序员就会在源码里面的各种地方加解锁。很容易出现死锁的情况，很麻烦，为了省事，直接给进入解释器的线程加了一把锁。

**加锁的优缺点**

- 加锁的好处：保证了解释器Cpython解释器的数据资源的安全。

- 加锁的缺陷：单个进程的多线程不能使用多核。

注释：Jpython解释器和Pypy解释器没有GIL锁

**现在为什么不去掉GIL锁？**

- 因为Cpython解释器所有的业务逻辑都是围绕单个线程去实现的，去掉这个GIL锁几乎不可能。

![](http://9017499461.linshutu.top/%E7%A8%8B%E5%BA%8F%E7%9A%84%E6%89%A7%E8%A1%8C%E6%B5%813.jpg)

总结：单个进程的多线程可以并发，但是不能利用多核，故不能并行；多个进程可以并发，并行。

### GIL与lock的区别

相同点：都是相同的锁，互斥锁。

不同点：

- GIL全局解释器锁，保护计时器内部的数据资源的安全，GIL锁上锁和释放无需手动操作。

- 自己代码中定义的互斥锁保护进程中的资源数据的安全，自定义的互斥锁必须自己手动上锁，释放锁。

GIL锁的由来

```python
因为Python解释器帮你自动定期进行内存回收，你可以理解为python解释器里有一个独立的线程，每过一段时间它起wake up做一次全局轮询看看哪些内存数据是可以被清空的，此时你自己的程序 里的线程和 py解释器自己的线程是并发运行的，假设你的线程删除了一个变量，py解释器的垃圾回收线程在清空这个变量的过程中的clearing时刻，可能一个其它线程正好又重新给这个还没来及得清空的内存空间赋值了，结果就有可能新赋值的数据被删除了，为了解决类似的问题，python解释器简单粗暴的加了锁，即当一个线程运行时，其它人都不能动，这样就解决了上述的问题，  这可以说是Python早期版本的遗留问题。
```

### 验证计算密集型和IO密集型的效率

**计算密集型验证**

实例代码

```python
#计算密集型：单个进程的多线程并发  VS 多个进程的并发并行
def task():
    count = 0
    for i in range(10000000):
        count += 1


if __name__ == '__main__':

    # # 多进程的并发,并行
    # start_time = time.time()
    # l1 = []
    # for i in range(4):
    #     p = Process(target=task,)
    #     l1.append(p)
    #     p.start()
    #
    # for p in l1:
    #     p.join()
    #
    # print(f'执行效率:{time.time()- start_time}')  #1.418208122253418


    # 多线程的并发
    start_time = time.time()
    l1 = []
    for i in range(4):
        p = Thread(target=task,)
        l1.append(p)
        p.start()

    for p in l1:
        p.join()

    print(f'执行效率:{time.time()- start_time}')  #2.5491583347320557

# 总结: 计算密集型: 多进程的并发并行效率高.

```

**IO密集型验证**

```python
#IO密集型：单个进程的多线程并发  VS  多个进程的并并发并行
def task():
    count = 0
    time.sleep(random.randint(1,3))
    count += 1

if __name__ == '__main__':

# # 多进程的并发,并行
#     start_time = time.time()
#     l1 = []
#     for i in range(50):
#         p = Process(target=task,)
#         l1.append(p)
#         p.start()
#
#     for p in l1:
#         p.join()
# 
#     print(f'执行效率:{time.time()- start_time}')  # 13.535850763320923

# 多线程的并发
    start_time = time.time()
    l1 = []
    for i in range(50):
        p = Thread(target=task,)
        l1.append(p)
        p.start()

    for p in l1:
        p.join()

    print(f'执行效率:{time.time()- start_time}')  # 3.0294392108917236
```

总结：

计算密集型：多进程的并发的效率更高

IO密集型：单个进程的多线程的并发效率高

### 多线程实现socket通信

server

```python
import socket
from threading import Thread


def communicate(conn, addr):
	while 1:
		try:
			from_client_data = conn.recv(1024)
			print(f"来自客户端{addr[1]}的消息：{from_client_data.decode('utf-8')}")
			to_client_data = input(">>>").strip()
			conn.send(to_client_data.encode('utf-8'))
		except Exception:
			break
		conn.close()


def _accept():
	server = socket.socket()
	server.bind(('127.0.0.1', 8848))
	server.listen(5)
	while 1:
		conn, addr = server.accept()
		t = Thread(target=communicate, args=(conn, addr))
		t.start()


if __name__ == '__main__':
	_accept()
```

client

```python
import socket
from threading import ThreadError

client = socket.socket()
client.connect(('127.0.0.1',8848))
while 1:
	try:
		to_client_data = input(">>>").strip()
		client.send(to_client_data.encode('utf-8'))
		from_server_datta = client.recv(1024)
		print (f"来自服务端的消息：{from_server_datta.decode('utf-8')}")
	except Exception:
		break
client.close()
```

总结：无论是多线程还是多进程，如果按照上面的写法，来一个客户端请求，我就开一个线程，来一个请求开一个线程。所以，在计算机允许的范围内，开启的线程数量越多越好。

### 进程池、线程池

一个容器，这个容器限制住我们开启线程的数量，比如4个，第一次肯定只能并发的处理4个任务，只要有任务完成，线程马上就会接下一个任务。（以时间换空间）

```python
from concurrent.futures import ProcessPoolExecutor,ThreadPoolExecutor
import os
import time
import random

def task():

	print (f"{os.getpid()}来了")
	time.sleep(random.randint(1,3))


if __name__ == '__main__':
	# t = ThreadPoolExecutor() #参数默认不写，开启的线程数就是cpu的核心数*5
	#获取cpu的核心数方法os.cpu_count
	t = ProcessPoolExecutor() #默认不写，开启的进程数就是cpu的核心数
	for i in range(80):
		t.submit(task)
```

