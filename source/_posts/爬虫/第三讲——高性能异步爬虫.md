---
title: 第三讲——高性能异步爬虫
id: 3
date: 2019-9-17 20:30:00
tags: 爬虫
comment: true
---

### 背景

其实爬虫的本质就是client发请求批量获取server的响应数据，如果我们有多个url待爬取，只用一个线程且采用串行的方式执行，那只能等待爬取一个结束后才能继续下一个，效率会非常低。需要强调的是：对于单线程下串行N个任务，并不完全等同于低效，如果这N个任务都是纯计算的任务，那么该线程对cpu的利用率仍然会很高，之所以单线程下串行多个爬虫任务低效，是因为爬虫任务是明显的IO密集型（阻塞）程序。那么该如何提高爬取性能呢？

### 进程、线程池爬虫

**同步调用：**提交一个任务之后就在原地等待任务结束，等到拿到任务的结果后再继续下一行代码的运行，对于IO密集型的爬虫任务来说，效率是非常低下的。

**解决同步调用方案之线程/进程池（可以适当的使用）**

- 优点：很多程序员可能会考虑使用“线程池”或“连接池”。“线程池”旨在减少创建和销毁线程的频率，其维持一定合理数量的线程，并让空闲的线程重新承担新的执行任务。可以很好的降低系统开销。
- 缺点：“线程池”和“连接池”技术也只是在一定程度上缓解了频繁创建和销毁线程带来的资源占用。而且，所谓“池”始终有其上限，当请求大大超过上限时，“池”构成的系统对外界的响应并不比没有池的时候效果好多少。所以使用“池”必须考虑其面临的响应规模，并根据响应规模调整“池”的大小。

<!----more---->

**实例**

需求：基于multiprocessing在的Pool爬取梨视频信息

```python
import requests
import random
from lxml import etree
import re
from fake_useragent import UserAgent
#安装fake-useragent库:pip install fake-useragent
#导入线程池模块
from multiprocessing import Pool
#实例化线程池对象
pool = Pool()
url = 'http://www.pearvideo.com/category_1'
#随机产生UA
ua = UserAgent().random
headers = {
    'User-Agent':ua
}
#获取首页页面数据
page_text = requests.get(url=url,headers=headers).text
#对获取的首页页面数据中的相关视频详情链接进行解析
tree = etree.HTML(page_text)
li_list = tree.xpath('//div[@id="listvideoList"]/ul/li')
detail_urls = []#存储二级页面的url
for li in li_list:
    detail_url = 'http://www.pearvideo.com/'+li.xpath('./div/a/@href')[0]
    title = li.xpath('.//div[@class="vervideo-title"]/text()')[0]
    detail_urls.append(detail_url)
vedio_urls = []#存储视频的url
for url in detail_urls:
    page_text = requests.get(url=url,headers=headers).text
    vedio_url = re.findall('srcUrl="(.*?)"',page_text,re.S)[0]
    vedio_urls.append(vedio_url) 
#使用线程池进行视频数据下载    
func_request = lambda link:requests.get(url=link,headers=headers).content
video_data_list = pool.map(func_request,vedio_urls)
#使用线程池进行视频数据保存
func_saveData = lambda data:save(data)
pool.map(func_saveData,video_data_list)
def save(data):
    fileName = str(random.randint(1,10000))+'.mp4'
    with open(fileName,'wb') as fp:
        fp.write(data)
        print(fileName+'已存储')
pool.close()
pool.join()
```

小结：**对应上例中的所面临的可能同时出现的上千甚至上万次的客户端请求，“线程池”或“连接池”或许可以缓解部分压力，但是不能解决所有问题。总之，多线程模型可以方便高效的解决小规模的服务请求，但面对大规模的服务请求，多线程模型也会遇到瓶颈，可以用非阻塞接口来尝试解决这个问题。**

### 异步协程爬虫

前言：上节说到的池子问题有一个问题就是IO阻塞，IO阻塞，无论是多进程还是多线程，在遇到IO阻塞时都会被操作系统强行剥夺走CPU的执行权限，程序的执行效率因此就降低了下来。

解决这一问题的关键在于，我们自己从应用程序级别检测IO阻塞然后切换到我们自己程序的其他任务执行，这样把我们程序的IO降到最低，我们的程序处于就绪态就会增多，以此来迷惑操作系统，操作系统便以为我们的程序是IO比较少的程序，从而会尽可能多的分配CPU给我们，这样也就达到了提升程序执行效率的目的。

在python3.4之后新增了asyncio模块，可以帮我们检测IO阻塞，然后实现异步IO。**注意：asyncio只能发tcp级别的请求，不能发http协议。**

**什么是异步IO**

所谓的异步IO，就是呵呵我们发起一个IO阻塞，却不用等待它结束，我们就可以去做其他的事情，当它结束时，我们就会得到通知。

**实现异步IO的方式**

单线程+异步协程实现异步IO操作。

**异步协程的用法**

从 Python 3.4 开始，Python 中加入了协程的概念，但这个版本的协程还是以生成器对象为基础的，在 Python 3.5 则增加了 async/await，使得协程的实现更加方便。首先我们需要了解下面几个概念：

- **event_loop：事件循环**，相当于一个无限循环，我们可以把一些函数注册到这个事件循环上，当满足某些条件的时候，函数就会被循环执行。程序是按照设定的顺序从头执行到尾，运行的次数也是完全按照设定。当在编写异步程序时，必然其中有部分程序的运行耗时是比较久的，需要先让出当前程序的控制权，让其在背后运行，让另一部分的程序先运行起来。当背后运行的程序完成后，也需要及时通知主程序已经完成任务可以进行下一步操作，但这个过程所需的时间是不确定的，需要主程序不断的监听状态，一旦收到了任务完成的消息，就开始进行下一步。loop就是这个持续不断的监视器。
- **coroutine：中文翻译叫协程**，在 Python 中常指代为协程对象类型，我们可以将协程对象注册到事件循环中，它会被事件循环调用。我们可以使用 async 关键字来定义一个方法，这个方法在调用时不会立即被执行，而是返回一个协程对象。
- **task：任务**，它是对协程对象的进一步封装，包含了任务的各个状态。
- **future：代表将来执行或还没有执行的任务**，实际上和 task 没有本质区别。

**定义一个协程**

```python
import asyncio
#定义协程函数
async def execute(x):
    print('Number:', x)
#协程对象
coroutine = execute(1)
print('Coroutine:', coroutine)
print('After calling execute')
#创建一个事件循环loop
loop = asyncio.get_event_loop()
#将事件注册到循环loop中
loop.run_until_complete(coroutine)
print('After calling loop')
结果：
Coroutine: 
After calling execute
Number: 1
After calling loop

#注意：async定义的方法就会变成一个无法直接执行的coroutine对象，必须要将它注册到事件循环中去才可以执行。
```

**task的使用**

上文我们还提到了 task，它是对 coroutine 对象的进一步封装，**它里面相比 coroutine 对象多了运行状态，比如 running、finished 等**，我们可以用这些状态来获取协程对象的执行情况。

在上面的例子中，当我们将 coroutine 对象传递给 run_until_complete() 方法的时候，实际上它进行了一个操作就是将 coroutine 封装成了 task 对象，我们也可以显式地进行声明，如下所示：

```python
import asyncio
async def execute(x):
    print('Number:', x)
    return x

coroutine = execute(1)
print('Coroutine:', coroutine)
print('After calling execute')

loop = asyncio.get_event_loop()
#这一步就是创建任务，首先得创建事件循环，然后再去创建任务
task = loop.create_task(coroutine)

print('Task:', task)
loop.run_until_complete(task)
print('Task:', task)
print('After calling loop')
```

**ensure_future()的使用**

另外定义 task 对象还有一种方式，就是直接通过 asyncio 的 ensure_future() 方法，返回结果也是 task 对象，这样的话我们就可以不借助于 loop 来定义，即使我们还没有声明 loop 也可以提前定义好 task 对象，写法如下：

```python
import asyncio
async def execute(x):
    print('Number:', x)
    return x
coroutine = execute(1)
print('Coroutine:', coroutine)
print('After calling execute')

#这一步就是直接使用asyncio调用该方法执行返回task对象
task = asyncio.ensure_future(coroutine)
print('Task:', task)

loop = asyncio.get_event_loop()
loop.run_until_complete(task)
print('Task:', task)
print('After calling loop')
```

小结：为什么说，task和future在本质上没有区别，原因就再这里，他们都是创建task任务的，之后一个需要借助loop循环而已。

**绑定回调**

```python
import asyncio
import requests
async def request():
    url = 'https://www.baidu.com'
    status = requests.get(url).status_code
    return status
def callback(task):
    print('Status:', task.result())
#协程对象
coroutine = request()
#创建任务
task = asyncio.ensure_future(coroutine)
#回调
task.add_done_callback(callback)
print('Task:', task)
#创建循环事件loop
loop = asyncio.get_event_loop()
#把task任务注册到循环事件loop中
loop.run_until_complete(task)
print('Task:', task)

分析：
我们将 callback() 方法传递给了封装好的 task 对象，这样当 task 执行完毕之后就可以调用 callback() 方法了，同时 task 对象还会作为参数传递给 callback() 方法，调用 task 对象的 result() 方法就可以获取返回结果了。
```

**多任务协程**

当我们想执行多次请求的时候，我们可以定义一个task列表，然后使用asyncio和**wait()方法**。

```python
import asyncio
import time
async def request(url):
    print('正在下载',url)
    #在异步协程中如果出现了同步模块相关的代码，那么就无法实现异步。
    time.sleep(2)
    print('下载完毕',url)
start = time.time()
urls = [
    'www.baidu.com',
    'www.sogou.com',
    'www.goubanjia.com'
]
#任务列表：存放多个任务对象
tasks = []
for url in urls:
    c = request(url)
    task = asyncio.ensure_future(c)
    tasks.append(task)
loop = asyncio.get_event_loop()
#需要将任务列表封装到wait中
loop.run_until_complete(asyncio.wait(stasks))
print(time.time()-start)
结果：
正在下载 www.baidu.com
下载完毕 www.baidu.com
正在下载 www.sogou.com
下载完毕 www.sogou.com
正在下载 www.goubanjia.com
下载完毕 www.goubanjia.com
6.009262800216675
```

分析上述的代码：在实现异步环节的编码中不可以出现非异步模块的代码，否则就无法实现真正的异步了，但是上述的代码中的time.sleep就是非异步模块中的代码。因此我们可以改下成下面的代码：

```python
import asyncio
import time
async def request(url):
    print('正在下载',url)
    #在异步协程中如果出现了同步模块相关的代码，那么就无法实现异步。
    # time.sleep(2)
    #当在asyncio中遇到阻塞操作必须进行手动挂起,这就是我们欺骗操作系统的操作，用户态切换cpu，就可以一致霸占cpu了
    await asyncio.sleep(2)
    print('下载完毕',url)
start = time.time()
urls = [
    'www.baidu.com',
    'www.sogou.com',
    'www.goubanjia.com'
]
#任务列表：存放多个任务对象
stasks = []
for url in urls:
    c = request(url)
    task = asyncio.ensure_future(c)
    stasks.append(task)
loop = asyncio.get_event_loop()
#需要将任务列表封装到wait中
loop.run_until_complete(asyncio.wait(stasks))
print(time.time()-start)
结果：
正在下载 www.baidu.com
正在下载 www.sogou.com
正在下载 www.goubanjia.com
下载完毕 www.baidu.com
下载完毕 www.sogou.com
下载完毕 www.goubanjia.com
2.0058767795562744
```

**但是在实际的网页爬取的时候，依旧是顺序执行每一个网页的请求的，用时还是很长，这是为什么呢？答案就是我们的requests模块是非异步的模块**，这个时候，想要实现真正的异步必须使用基于异步网络请求模块，它就是**aiohttp**。

**aiohttp**

它可以实现单线程并发IO操作

**aiohttp的使用**

- 发起请求

```python
async def fetch():
	async with aiohttp.ClientSession() as session:
        async with session.get("https://baidu.com") as response:
            print (await response.text())
loop = asyncio.get_event_loop()
tasks = [fetch(),]
loop.run_until_complete(asyncio.wait(tasks))
```

- 添加请求参数

```python
params = {'key': 'value', 'page': 10}
async def fetch():
    async with aiohttp.ClientSession() as session:
        async with session.get('https://www.baidu.com/s',params=params) as resposne:
            print(await resposne.url)
loop = asyncio.get_event_loop()
tasks = [fetch(),]
loop.run_until_complete(asyncio.wait(tasks))
```

- UA伪装

```python
url = 'http://httpbin.org/user-agent'
headers = {'User-Agent': 'test_user_agent'}
async def fetch():
    async with aiohttp.ClientSession() as session:
        async with session.get(url,headers=headers) as resposne:
            print(await resposne.text())
loop = asyncio.get_event_loop()
tasks = [fetch(),]
loop.run_until_complete(asyncio.wait(tasks))
```

- 自定义cookies

```python
url = 'http://httpbin.org/cookies'
cookies = {'cookies_name': 'test_cookies'}
async def fetch():
    async with aiohttp.ClientSession() as session:
        async with session.get(url,cookies=cookies) as resposne:
            print(await resposne.text())
loop = asyncio.get_event_loop()
tasks = [fetch(),]
loop.run_until_complete(asyncio.wait(tasks))
```

- post请求参数

```python
url = 'http://httpbin.org'
payload = {'username': 'zhang', 'password': '123456'}
async def fetch():
    async with aiohttp.ClientSession() as session:
        async with session.post(url, data=payload) as resposne:
            print(await resposne.text())
loop = asyncio.get_event_loop()
tasks = [fetch(), ]
loop.run_until_complete(asyncio.wait(tasks))
```

- 设置代理

```python
url = "http://python.org"
async def fetch():
    async with aiohttp.ClientSession() as session:
        async with session.get(url, proxy="http://some.proxy.com") as resposne:
        print(resposne.status)
loop = asyncio.get_event_loop()
tasks = [fetch(), ]
loop.run_until_complete(asyncio.wait(tasks))
```

**真正的异步IO处理**

```python
#使用该模块中的ClientSession
import requests
import asyncio
import time
import aiohttp
start = time.time()
urls = [
    'http://127.0.0.1:5000/bobo','http://127.0.0.1:5000/jay','http://127.0.0.1:5000/tom',
    'http://127.0.0.1:5000/bobo', 'http://127.0.0.1:5000/jay', 'http://127.0.0.1:5000/tom',
    'http://127.0.0.1:5000/bobo', 'http://127.0.0.1:5000/jay', 'http://127.0.0.1:5000/tom',
    'http://127.0.0.1:5000/bobo', 'http://127.0.0.1:5000/jay', 'http://127.0.0.1:5000/tom',
]
async def get_page(url):
    async with aiohttp.ClientSession() as session:
        #get()、post():
        #headers,params/data,proxy='http://ip:port'
        async with await session.get(url) as response:
            #text()返回字符串形式的响应数据
            #read()返回的二进制形式的响应数据
            #json()返回的就是json对象
            #注意：获取响应数据操作之前一定要使用await进行手动挂起
            page_text = await response.text()
            print(page_text)
tasks = []
for url in urls:
    c = get_page(url)
    task = asyncio.ensure_future(c)
    tasks.append(task)
loop = asyncio.get_event_loop()
loop.run_until_complete(asyncio.wait(tasks))
end = time.time()
print('总耗时:',end-start)
结果：
Hello tom
Hello jay
Hello bobo
Hello bobo
Hello jay
Hello bobo
Hello tom
Hello jay
Hello jay
Hello tom
Hello tom
Hello bobo
总耗时: 2.037203073501587
我们发现这次请求的耗时由 6秒变成了 2 秒，耗时直接变成了原来的1/3。
```

分析代码：在代码中，我们使用了await，后面跟着get()方法，在执行这五个协程的时候，如果遇到了await，那么就会将当前协程挂起，转而去执行其他的协程，直到其他的协程执行完毕或者也挂起，再进行下一个协程的执行。

外链：gevent中的协程：https://chpl.top/2019/08/27/%E4%B9%A6/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E4%B8%8E%E5%B9%B6%E5%8F%91/%E7%AC%AC%E4%B8%89%E5%8D%81%E5%85%AD%E8%AE%B2%E2%80%94%E2%80%94%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E4%B9%8B%E5%8D%8F%E7%A8%8B/